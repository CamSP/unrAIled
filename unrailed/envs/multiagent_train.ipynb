{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ROCK' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m MOVES \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mUP\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDOWN\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mRIGHT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLEFT\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mGRAP\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mBREAK\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m NUM_ITERS \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m REWARD_MAP \u001b[39m=\u001b[39m {\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     (ROCK, ROCK): (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     (ROCK, PAPER): (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     (ROCK, SCISSORS): (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     (PAPER, ROCK): (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     (PAPER, PAPER): (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     (PAPER, SCISSORS): (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     (SCISSORS, ROCK): (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     (SCISSORS, PAPER): (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     (SCISSORS, SCISSORS): (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39menv\u001b[39m(render_mode\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m    The env function often wraps the environment in wrappers by default.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m    You can find full documentation for these methods\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m    elsewhere in the developer documentation.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ROCK' is not defined"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import error, spaces, utils\n",
    "from gymnasium.utils import seeding\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "import os\n",
    "from pygame.surfarray import array3d\n",
    "from pygame import display\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers\n",
    "\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "GRAP = 4\n",
    "BREAK = 5\n",
    "\n",
    "MOVES = [\"UP\", \"DOWN\", \"RIGHT\", \"LEFT\", \"GRAP\", \"BREAK\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def raw_env(render_mode=None):\n",
    "    \"\"\"\n",
    "    To support the AEC API, the raw_env() function just uses the from_parallel\n",
    "    function to convert from a ParallelEnv to an AEC env\n",
    "    \"\"\"\n",
    "    env = UnrailedEnv(render_mode=render_mode)\n",
    "    env = parallel_to_aec(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "[0, 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 2\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpettingzoo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtest\u001b[39;00m \u001b[39mimport\u001b[39;00m parallel_api_test\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munrailed_env2\u001b[39;00m \u001b[39mimport\u001b[39;00m UnrailedEnv \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m UnrailedEnv()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m parallel_api_test(env, num_cycles\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n",
      "File \u001b[0;32m~/Escritorio/Optimizacion/unrAIled/unrailed/envs/unrailed_env2.py:282\u001b[0m, in \u001b[0;36mUnrailedEnv.__init__\u001b[0;34m(self, render_mode, curriculum)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mwood_item\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mwood_item\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mvision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mvision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetAgentView(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m25\u001b[39m))[\u001b[39m0\u001b[39m]),\n\u001b[0;32m--> 282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservations[agent][\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_args[\u001b[39m\"\u001b[39;49m\u001b[39mposition\u001b[39;49m\u001b[39m\"\u001b[39;49m]] \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m1\u001b[39m]],\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m2\u001b[39m],\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "from pettingzoo.test import parallel_api_test\n",
    "from unrailed_env2 import UnrailedEnv \n",
    "env = UnrailedEnv()\n",
    "parallel_api_test(env, num_cycles=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25,)\n",
      "(25,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munrailed_env2\u001b[39;00m \u001b[39mimport\u001b[39;00m UnrailedEnv \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m env \u001b[39m=\u001b[39m UnrailedEnv()\n",
      "File \u001b[0;32m~/Escritorio/Optimizacion/unrAIled/unrailed/envs/unrailed_env2.py:281\u001b[0m, in \u001b[0;36mUnrailedEnv.__init__\u001b[0;34m(self, render_mode, curriculum)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetAgentView(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    280\u001b[0m \u001b[39mfor\u001b[39;00m i, agent \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents):\n\u001b[0;32m--> 281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservations[agent][\u001b[39m0\u001b[39;49m][\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_args[\u001b[39m\"\u001b[39;49m\u001b[39mvision\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m0\u001b[39;49m]:\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_args[\u001b[39m\"\u001b[39;49m\u001b[39mvision\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m1\u001b[39;49m]] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetAgentView(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m1\u001b[39m])),\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39m0\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m1\u001b[39m]],\n\u001b[1;32m    283\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39m0\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m2\u001b[39m],\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import supersuit as ss\n",
    "import torch\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.hpo.mutation import Mutations\n",
    "from agilerl.hpo.tournament import TournamentSelection\n",
    "from agilerl.utils.utils import initialPopulation\n",
    "from tqdm import trange\n",
    "\n",
    "from pettingzoo.atari import space_invaders_v2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from unrailed_env2 import UnrailedEnv \n",
    "env = UnrailedEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "[0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munrailed_env2\u001b[39;00m \u001b[39mimport\u001b[39;00m UnrailedEnv \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgymnasium\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m \u001b[39mimport\u001b[39;00m FlattenObservation\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m env \u001b[39m=\u001b[39m UnrailedEnv()\n",
      "File \u001b[0;32m~/Escritorio/Optimizacion/unrAIled/unrailed/envs/unrailed_env2.py:282\u001b[0m, in \u001b[0;36mUnrailedEnv.__init__\u001b[0;34m(self, render_mode, curriculum)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mwood_item\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mwood_item\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]])\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mvision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mvision\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetAgentView(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mreshape((\u001b[39m1\u001b[39m, \u001b[39m25\u001b[39m))[\u001b[39m0\u001b[39m]),\n\u001b[0;32m--> 282\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservations[agent][\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobs_args[\u001b[39m\"\u001b[39;49m\u001b[39mposition\u001b[39;49m\u001b[39m\"\u001b[39;49m]] \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m1\u001b[39m]],\n\u001b[1;32m    283\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents_pos[i][\u001b[39m2\u001b[39m],\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservations[agent][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_args[\u001b[39m\"\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m\"\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "from unrailed_env2 import UnrailedEnv \n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "env = UnrailedEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space = env.observation_space(\"player_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "a[1:3] = [1, 2]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, [1. 1. 1. ... 1. 1. 1.], (1015,), float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_items([('axe', MultiDiscrete([45 20])), ('bucket', MultiDiscrete([45 20])), ('deposit', MultiDiscrete([45 20])), ('factory', MultiDiscrete([45 20])), ('full_bucket', MultiDiscrete([45 20])), ('house', MultiDiscrete([45 20])), ('item', Discrete(7)), ('orientation', Discrete(4)), ('pickaxe', MultiDiscrete([45 20])), ('position', MultiDiscrete([45 20])), ('refri', MultiDiscrete([45 20])), ('riel', MultiDiscrete([45 20])), ('rock_block', MultiDiscrete([45 20])), ('rock_item', MultiDiscrete([45 20])), ('task', Discrete(4)), ('tree_block', MultiDiscrete([45 20])), ('vision', Box(0.0, 18.0, (5, 5), float32)), ('water', MultiDiscrete([45 20])), ('wood_item', MultiDiscrete([45 20]))])\n",
      "{'player_0': {'vision': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]]), 'position': [9, 16], 'orientation': 0, 'item': 0, 'wood_item': (-1, -1), 'rock_item': (-1, -1), 'pickaxe': (6, 11), 'axe': (10, 9), 'bucket': (-1, -1), 'full_bucket': (12, 13), 'rock_block': (12, 5), 'tree_block': (5, 4), 'water': (15, 19), 'refri': (9, 4), 'deposit': (9, 2), 'factory': (9, 3), 'riel': (9, 8), 'house': (7, 42), 'task': 0, 'view': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]])}, 'player_1': {'vision': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]]), 'position': [10, 17], 'orientation': 0, 'item': 0, 'wood_item': (-1, -1), 'rock_item': (-1, -1), 'pickaxe': (6, 11), 'axe': (10, 9), 'bucket': (-1, -1), 'full_bucket': (12, 13), 'rock_block': (12, 5), 'tree_block': (5, 4), 'water': (15, 19), 'refri': (9, 4), 'deposit': (9, 2), 'factory': (9, 3), 'riel': (9, 8), 'house': (7, 42), 'task': 0, 'view': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]])}, 'player_2': {'vision': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]]), 'position': [11, 16], 'orientation': 0, 'item': 0, 'wood_item': (-1, -1), 'rock_item': (-1, -1), 'pickaxe': (6, 11), 'axe': (10, 9), 'bucket': (-1, -1), 'full_bucket': (12, 13), 'rock_block': (12, 5), 'tree_block': (5, 4), 'water': (15, 19), 'refri': (9, 4), 'deposit': (9, 2), 'factory': (9, 3), 'riel': (9, 8), 'house': (7, 42), 'task': 0, 'view': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]])}, 'player_3': {'vision': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]]), 'position': [12, 15], 'orientation': 0, 'item': 0, 'wood_item': (-1, -1), 'rock_item': (-1, -1), 'pickaxe': (6, 11), 'axe': (10, 9), 'bucket': (-1, -1), 'full_bucket': (12, 13), 'rock_block': (12, 5), 'tree_block': (5, 4), 'water': (15, 19), 'refri': (9, 4), 'deposit': (9, 2), 'factory': (9, 3), 'riel': (9, 8), 'house': (7, 42), 'task': 0, 'view': array([[1, 1, 1],\n",
      "       [1, 1, 1],\n",
      "       [1, 1, 1]])}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'axe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39;49mstep({\u001b[39m'\u001b[39;49m\u001b[39mplayer_0\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m4\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mplayer_1\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m3\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mplayer_2\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m4\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mplayer_3\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m2\u001b[39;49m}))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/core.py:470\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m    469\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m--> 470\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/wrappers/flatten_observation.py:43\u001b[0m, in \u001b[0;36mFlattenObservation.observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m     35\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Flattens an observation.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m        The flattened observation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m spaces\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mobservation_space, observation)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/utils.py:198\u001b[0m, in \u001b[0;36m_flatten_dict\u001b[0;34m(space, x)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mprint\u001b[39m(space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    196\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m--> 198\u001b[0m         [np\u001b[39m.\u001b[39marray(flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/utils.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mprint\u001b[39m(space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n\u001b[1;32m    196\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m--> 198\u001b[0m         [np\u001b[39m.\u001b[39marray(flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    199\u001b[0m     )\n\u001b[1;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'axe'"
     ]
    }
   ],
   "source": [
    "print(env.step({'player_0': 4, 'player_1': 3, 'player_2': 4, 'player_3': 2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TASK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#Y100sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mreset()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/core.py:463\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`reset`, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[1;32m    462\u001b[0m obs, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39mseed, options\u001b[39m=\u001b[39moptions)\n\u001b[0;32m--> 463\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation(obs), info\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/wrappers/flatten_observation.py:43\u001b[0m, in \u001b[0;36mFlattenObservation.observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m     35\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Flattens an observation.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m        The flattened observation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m spaces\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mobservation_space, observation)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m_flatten_dict\u001b[0;34m(space, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@flatten\u001b[39m\u001b[39m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_dict\u001b[39m(space: Dict, x: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any] \u001b[39m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m space\u001b[39m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[39m.\u001b[39marray(flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@flatten\u001b[39m\u001b[39m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_dict\u001b[39m(space: Dict, x: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any] \u001b[39m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m space\u001b[39m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[39m.\u001b[39marray(flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TASK'"
     ]
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1015,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== AgileRL MADDPG Demo =====\n"
     ]
    }
   ],
   "source": [
    "print(\"===== AgileRL MADDPG Demo =====\")\n",
    "\n",
    "# Define the network configuration\n",
    "NET_CONFIG = {\n",
    "    \"arch\": \"mlp\",  # Network architecture\n",
    "    \"h_size\": [32, 32],  # Network hidden size\n",
    "}\n",
    "\n",
    "# Define the initial hyperparameters\n",
    "INIT_HP = {\n",
    "    \"POPULATION_SIZE\": 4,\n",
    "    \"ALGO\": \"MADDPG\",  # Algorithm\n",
    "    \"BATCH_SIZE\": 8,  # Batch size\n",
    "    \"LR\": 0.01,  # Learning rate\n",
    "    \"GAMMA\": 0.95,  # Discount factor\n",
    "    \"MEMORY_SIZE\": 10000,  # Max memory buffer size\n",
    "    \"LEARN_STEP\": 5,  # Learning frequency\n",
    "    \"TAU\": 0.01,  # For soft update of target parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure the multi-agent algo input arguments\n",
    "try:\n",
    "    state_dim = [env.observation_space.n for agent in env.agents]\n",
    "    one_hot = True\n",
    "except Exception:\n",
    "    state_dim = [env.observation_space.shape for agent in env.agents]\n",
    "    one_hot = False\n",
    "    \n",
    "try:\n",
    "    action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "    INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "    INIT_HP[\"MAX_ACTION\"] = None\n",
    "    INIT_HP[\"MIN_ACTION\"] = None\n",
    "except Exception:\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "    INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "    INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "# Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "INIT_HP[\"AGENT_IDS\"] = env.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a population ready for evolutionary hyper-parameter optimisation\n",
    "pop = initialPopulation(\n",
    "    INIT_HP[\"ALGO\"],\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    one_hot,\n",
    "    NET_CONFIG,\n",
    "    INIT_HP,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Configure the multi-agent replay buffer\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Instantiate a tournament selection object (used for HPO)\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,  # Tournament selection size\n",
    "    elitism=True,  # Elitism in tournament selection\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "    evo_step=1,\n",
    ")  # Evaluate using last N fitness scores\n",
    "\n",
    "# Instantiate a mutations object (used for HPO)\n",
    "mutations = Mutations(\n",
    "    algo=INIT_HP[\"ALGO\"],\n",
    "    no_mutation=0.2,  # Probability of no mutation\n",
    "    architecture=0.2,  # Probability of architecture mutation\n",
    "    new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "    parameters=0.2,  # Probability of parameter mutation\n",
    "    activation=0,  # Probability of activation function mutation\n",
    "    rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "    rl_hp_selection=[\n",
    "        \"lr\",\n",
    "        \"learn_step\",\n",
    "        \"batch_size\",\n",
    "    ],  # RL hyperparams selected for mutation\n",
    "    mutation_sd=0.1,  # Mutation strength\n",
    "    # Define search space for each hyperparameter\n",
    "    min_lr=0.0001,\n",
    "    max_lr=0.01,\n",
    "    min_learn_step=1,\n",
    "    max_learn_step=120,\n",
    "    min_batch_size=8,\n",
    "    max_batch_size=64,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],  # Agent IDs\n",
    "    arch=NET_CONFIG[\"arch\"],  # MLP or CNN\n",
    "    rand_seed=1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Define training loop parameters\n",
    "max_episodes = 1000  # Total episodes (default: 6000)\n",
    "max_steps = 900  # Maximum steps to take in each episode\n",
    "epsilon = 1.0  # Starting epsilon value\n",
    "eps_end = 0.1  # Final epsilon value\n",
    "eps_decay = 0.995  # Epsilon decay\n",
    "evo_epochs = 20  # Evolution frequency\n",
    "evo_loop = 1  # Number of evaluation episodes\n",
    "elite = pop[0]  # Assign a placeholder \"elite\" agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'TASK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx_epi \u001b[39min\u001b[39;00m trange(max_episodes):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m pop:  \u001b[39m# Loop through population\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         state, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()  \u001b[39m# Reset environment at start of episode\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         agent_reward \u001b[39m=\u001b[39m {agent_id: \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m agent_id \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39magents}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X52sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_steps):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/core.py:463\u001b[0m, in \u001b[0;36mObservationWrapper.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`reset`, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[39;00m\n\u001b[1;32m    462\u001b[0m obs, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39mseed, options\u001b[39m=\u001b[39moptions)\n\u001b[0;32m--> 463\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation(obs), info\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/wrappers/flatten_observation.py:43\u001b[0m, in \u001b[0;36mFlattenObservation.observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m     35\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Flattens an observation.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m        The flattened observation\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m spaces\u001b[39m.\u001b[39;49mflatten(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mobservation_space, observation)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mfuncname\u001b[39m}\u001b[39;00m\u001b[39m requires at least \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39m1 positional argument\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[39mreturn\u001b[39;00m dispatch(args[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m_flatten_dict\u001b[0;34m(space, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@flatten\u001b[39m\u001b[39m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_dict\u001b[39m(space: Dict, x: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any] \u001b[39m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m space\u001b[39m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[39m.\u001b[39marray(flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/spaces/utils.py:196\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@flatten\u001b[39m\u001b[39m.\u001b[39mregister(Dict)\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flatten_dict\u001b[39m(space: Dict, x: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any] \u001b[39m|\u001b[39m NDArray[Any]:\n\u001b[1;32m    194\u001b[0m     \u001b[39mif\u001b[39;00m space\u001b[39m.\u001b[39mis_np_flattenable:\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[0;32m--> 196\u001b[0m             [np\u001b[39m.\u001b[39marray(flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict((key, flatten(s, x[key])) \u001b[39mfor\u001b[39;00m key, s \u001b[39min\u001b[39;00m space\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyError\u001b[0m: 'TASK'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "for idx_epi in trange(max_episodes):\n",
    "    for agent in pop:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "        if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "            state = {\n",
    "                agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                for agent_id, s in state.items()\n",
    "            }\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = agent.getAction(\n",
    "                state, epsilon, agent_mask, env_defined_actions\n",
    "            )\n",
    "            if agent.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            next_state, reward, termination, truncation, info = env.step(\n",
    "                action\n",
    "            )  # Act in environment\n",
    "\n",
    "            # Image processing if necessary for the environment\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                next_state = {\n",
    "                    agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                    for agent_id, ns in next_state.items()\n",
    "                }\n",
    "\n",
    "            # Save experiences to replay buffer\n",
    "            memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "            # Collect the reward\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Learn according to learning frequency\n",
    "            if (memory.counter % agent.learn_step == 0) and (\n",
    "                len(memory) >= agent.batch_size\n",
    "            ):\n",
    "                experiences = memory.sample(\n",
    "                    agent.batch_size\n",
    "                )  # Sample replay buffer\n",
    "                agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "            # Update the state\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                next_state = {\n",
    "                    agent_id: np.expand_dims(ns, 0)\n",
    "                    for agent_id, ns in next_state.items()\n",
    "                }\n",
    "            state = next_state\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        # Save the total episode reward\n",
    "        score = sum(agent_reward.values())\n",
    "        agent.scores.append(score)\n",
    "\n",
    "    # Update epsilon for exploration\n",
    "    epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "    # Now evolve population if necessary\n",
    "    if (idx_epi + 1) % evo_epochs == 0:\n",
    "        # Evaluate population\n",
    "        fitnesses = [\n",
    "            agent.test(\n",
    "                env,\n",
    "                swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "                max_steps=max_steps,\n",
    "                loop=evo_loop,\n",
    "            )\n",
    "            for agent in pop\n",
    "        ]\n",
    "\n",
    "        print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "        print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "        print(\n",
    "            f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in pop]}'\n",
    "        )\n",
    "\n",
    "        # Tournament selection and population mutation\n",
    "        elite, pop = tournament.select(pop)\n",
    "        pop = mutations.mutation(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained algorithm\n",
    "path = \"./models/MADDPG\"\n",
    "filename = \"MADDPG_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "elite.saveCheckpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained algorithm\n",
    "path = \"./models/MADDPG\"\n",
    "filename = \"MADDPG_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "elite.saveCheckpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[env.observation_space(agent).shape for agent in env.agents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dict' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X56sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mobservation_space(\u001b[39m\"\u001b[39;49m\u001b[39mplayer_1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Escritorio/Optimizacion/unrAIled/unrailed/envs/unrailed_env2.py:305\u001b[0m, in \u001b[0;36mUnrailedEnv.observation_space\u001b[0;34m(self, agent)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(maxsize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    283\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobservation_space\u001b[39m(\u001b[39mself\u001b[39m, agent):\n\u001b[1;32m    284\u001b[0m     obs \u001b[39m=\u001b[39m Dict({\n\u001b[1;32m    285\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvision\u001b[39m\u001b[39m\"\u001b[39m: Box(low\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, high\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, shape\u001b[39m=\u001b[39m(\u001b[39m5\u001b[39m,\u001b[39m5\u001b[39m)),\n\u001b[1;32m    286\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m\"\u001b[39m: MultiDiscrete([\u001b[39m45\u001b[39m, \u001b[39m20\u001b[39m]),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTASK\u001b[39m\u001b[39m\"\u001b[39m: Discrete(\u001b[39m4\u001b[39m)\n\u001b[1;32m    304\u001b[0m     })\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mreturn\u001b[39;00m FlattenObservation(obs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/gymnasium/wrappers/flatten_observation.py:32\u001b[0m, in \u001b[0;36mFlattenObservation.__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     29\u001b[0m gym\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mRecordConstructorArgs\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m     30\u001b[0m gym\u001b[39m.\u001b[39mObservationWrapper\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env)\n\u001b[0;32m---> 32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39mflatten_space(env\u001b[39m.\u001b[39;49mobservation_space)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dict' object has no attribute 'observation_space'"
     ]
    }
   ],
   "source": [
    "env.observation_space(\"player_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MultiDiscrete' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39;49m(env\u001b[39m.\u001b[39;49mobservation_space(\u001b[39m\"\u001b[39;49m\u001b[39mplayer_1\u001b[39;49m\u001b[39m\"\u001b[39;49m)[item]\u001b[39m.\u001b[39;49mn \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m env\u001b[39m.\u001b[39;49mobservation_space(\u001b[39m\"\u001b[39;49m\u001b[39mplayer_1\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n",
      "\u001b[1;32m/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/camilo/Escritorio/Optimizacion/unrAIled/unrailed/envs/multiagent_train.ipynb#X55sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39msum\u001b[39m(env\u001b[39m.\u001b[39;49mobservation_space(\u001b[39m\"\u001b[39;49m\u001b[39mplayer_1\u001b[39;49m\u001b[39m\"\u001b[39;49m)[item]\u001b[39m.\u001b[39;49mn \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mobservation_space(\u001b[39m\"\u001b[39m\u001b[39mplayer_1\u001b[39m\u001b[39m\"\u001b[39m)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiDiscrete' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "print(sum(env.observation_space(\"player_1\")[item].n for item in env.observation_space(\"player_1\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the network configuration\n",
    "NET_CONFIG = {\n",
    "    \"arch\": \"cnn\",  # Network architecture\n",
    "    \"h_size\": [32, 32],  # Network hidden size\n",
    "    \"c_size\": [3, 32],  # CNN channel size\n",
    "    \"k_size\": [(1, 3, 3), (1, 3, 3)],  # CNN kernel size\n",
    "    \"s_size\": [2, 2],  # CNN stride size\n",
    "    \"normalize\": True,  # Normalize image from range [0,255] to [0,1]\n",
    "}\n",
    "\n",
    "# Define the initial hyperparameters\n",
    "INIT_HP = {\n",
    "    \"POPULATION_SIZE\": 4,\n",
    "    \"ALGO\": \"MADDPG\",  # Algorithm\n",
    "    \"BATCH_SIZE\": 8,  # Batch size\n",
    "    \"LR\": 0.01,  # Learning rate\n",
    "    \"GAMMA\": 0.95,  # Discount factor\n",
    "    \"MEMORY_SIZE\": 10000,  # Max memory buffer size\n",
    "    \"LEARN_STEP\": 5,  # Learning frequency\n",
    "    \"TAU\": 0.01,  # For soft update of target parameters\n",
    "}\n",
    "\n",
    "# Configure the multi-agent algo input arguments\n",
    "try:\n",
    "    state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "    one_hot = True\n",
    "except Exception:\n",
    "    state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "    one_hot = False\n",
    "    \n",
    "try:\n",
    "    action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "    INIT_HP[\"DISCRETE_ACTIONS\"] = True\n",
    "    INIT_HP[\"MAX_ACTION\"] = None\n",
    "    INIT_HP[\"MIN_ACTION\"] = None\n",
    "except Exception:\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    INIT_HP[\"DISCRETE_ACTIONS\"] = False\n",
    "    INIT_HP[\"MAX_ACTION\"] = [env.action_space(agent).high for agent in env.agents]\n",
    "    INIT_HP[\"MIN_ACTION\"] = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "# Pre-process image dimensions for pytorch convolutional layers\n",
    "if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "    state_dim = [\n",
    "        (state_dim[2], state_dim[0], state_dim[1]) for state_dim in state_dim\n",
    "    ]\n",
    "\n",
    "# Append number of agents and agent IDs to the initial hyperparameter dictionary\n",
    "INIT_HP[\"N_AGENTS\"] = env.num_agents\n",
    "INIT_HP[\"AGENT_IDS\"] = env.agents\n",
    "\n",
    "# Create a population ready for evolutionary hyper-parameter optimisation\n",
    "pop = initialPopulation(\n",
    "    INIT_HP[\"ALGO\"],\n",
    "    state_dim,\n",
    "    action_dim,\n",
    "    one_hot,\n",
    "    NET_CONFIG,\n",
    "    INIT_HP,\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Configure the multi-agent replay buffer\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    INIT_HP[\"MEMORY_SIZE\"],\n",
    "    field_names=field_names,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Instantiate a tournament selection object (used for HPO)\n",
    "tournament = TournamentSelection(\n",
    "    tournament_size=2,  # Tournament selection size\n",
    "    elitism=True,  # Elitism in tournament selection\n",
    "    population_size=INIT_HP[\"POPULATION_SIZE\"],  # Population size\n",
    "    evo_step=1,\n",
    ")  # Evaluate using last N fitness scores\n",
    "\n",
    "# Instantiate a mutations object (used for HPO)\n",
    "mutations = Mutations(\n",
    "    algo=INIT_HP[\"ALGO\"],\n",
    "    no_mutation=0.2,  # Probability of no mutation\n",
    "    architecture=0.2,  # Probability of architecture mutation\n",
    "    new_layer_prob=0.2,  # Probability of new layer mutation\n",
    "    parameters=0.2,  # Probability of parameter mutation\n",
    "    activation=0,  # Probability of activation function mutation\n",
    "    rl_hp=0.2,  # Probability of RL hyperparameter mutation\n",
    "    rl_hp_selection=[\n",
    "        \"lr\",\n",
    "        \"learn_step\",\n",
    "        \"batch_size\",\n",
    "    ],  # RL hyperparams selected for mutation\n",
    "    mutation_sd=0.1,  # Mutation strength\n",
    "    # Define search space for each hyperparameter\n",
    "    min_lr=0.0001,\n",
    "    max_lr=0.01,\n",
    "    min_learn_step=1,\n",
    "    max_learn_step=120,\n",
    "    min_batch_size=8,\n",
    "    max_batch_size=64,\n",
    "    agent_ids=INIT_HP[\"AGENT_IDS\"],  # Agent IDs\n",
    "    arch=NET_CONFIG[\"arch\"],  # MLP or CNN\n",
    "    rand_seed=1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Define training loop parameters\n",
    "max_episodes = 5  # Total episodes (default: 6000)\n",
    "max_steps = 900  # Maximum steps to take in each episode\n",
    "epsilon = 1.0  # Starting epsilon value\n",
    "eps_end = 0.1  # Final epsilon value\n",
    "eps_decay = 0.995  # Epsilon decay\n",
    "evo_epochs = 20  # Evolution frequency\n",
    "evo_loop = 1  # Number of evaluation episodes\n",
    "elite = pop[0]  # Assign a placeholder \"elite\" agent\n",
    "\n",
    "# Training loop\n",
    "for idx_epi in trange(max_episodes):\n",
    "    for agent in pop:  # Loop through population\n",
    "        state, info = env.reset()  # Reset environment at start of episode\n",
    "        agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "        if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "            state = {\n",
    "                agent_id: np.moveaxis(np.expand_dims(s, 0), [-1], [-3])\n",
    "                for agent_id, s in state.items()\n",
    "            }\n",
    "        for _ in range(max_steps):\n",
    "            agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "            env_defined_actions = (\n",
    "                info[\"env_defined_actions\"]\n",
    "                if \"env_defined_actions\" in info.keys()\n",
    "                else None\n",
    "            )\n",
    "\n",
    "            # Get next action from agent\n",
    "            cont_actions, discrete_action = agent.getAction(\n",
    "                state, epsilon, agent_mask, env_defined_actions\n",
    "            )\n",
    "            if agent.discrete_actions:\n",
    "                action = discrete_action\n",
    "            else:\n",
    "                action = cont_actions\n",
    "\n",
    "            next_state, reward, termination, truncation, info = env.step(\n",
    "                action\n",
    "            )  # Act in environment\n",
    "\n",
    "            # Image processing if necessary for the environment\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                state = {agent_id: np.squeeze(s) for agent_id, s in state.items()}\n",
    "                next_state = {\n",
    "                    agent_id: np.moveaxis(ns, [-1], [-3])\n",
    "                    for agent_id, ns in next_state.items()\n",
    "                }\n",
    "\n",
    "            # Save experiences to replay buffer\n",
    "            memory.save2memory(state, cont_actions, reward, next_state, termination)\n",
    "\n",
    "            # Collect the reward\n",
    "            for agent_id, r in reward.items():\n",
    "                agent_reward[agent_id] += r\n",
    "\n",
    "            # Learn according to learning frequency\n",
    "            if (memory.counter % agent.learn_step == 0) and (\n",
    "                len(memory) >= agent.batch_size\n",
    "            ):\n",
    "                experiences = memory.sample(\n",
    "                    agent.batch_size\n",
    "                )  # Sample replay buffer\n",
    "                agent.learn(experiences)  # Learn according to agent's RL algorithm\n",
    "\n",
    "            # Update the state\n",
    "            if INIT_HP[\"CHANNELS_LAST\"]:\n",
    "                next_state = {\n",
    "                    agent_id: np.expand_dims(ns, 0)\n",
    "                    for agent_id, ns in next_state.items()\n",
    "                }\n",
    "            state = next_state\n",
    "\n",
    "            # Stop episode if any agents have terminated\n",
    "            if any(truncation.values()) or any(termination.values()):\n",
    "                break\n",
    "\n",
    "        # Save the total episode reward\n",
    "        score = sum(agent_reward.values())\n",
    "        agent.scores.append(score)\n",
    "\n",
    "    # Update epsilon for exploration\n",
    "    epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "    # Now evolve population if necessary\n",
    "    if (idx_epi + 1) % evo_epochs == 0:\n",
    "        # Evaluate population\n",
    "        fitnesses = [\n",
    "            agent.test(\n",
    "                env,\n",
    "                swap_channels=INIT_HP[\"CHANNELS_LAST\"],\n",
    "                max_steps=max_steps,\n",
    "                loop=evo_loop,\n",
    "            )\n",
    "            for agent in pop\n",
    "        ]\n",
    "\n",
    "        print(f\"Episode {idx_epi + 1}/{max_episodes}\")\n",
    "        print(f'Fitnesses: {[\"%.2f\" % fitness for fitness in fitnesses]}')\n",
    "        print(\n",
    "            f'100 fitness avgs: {[\"%.2f\" % np.mean(agent.fitness[-100:]) for agent in pop]}'\n",
    "        )\n",
    "\n",
    "        # Tournament selection and population mutation\n",
    "        elite, pop = tournament.select(pop)\n",
    "        pop = mutations.mutation(pop)\n",
    "\n",
    "# Save the trained algorithm\n",
    "path = \"./models/MADDPG\"\n",
    "filename = \"MADDPG_trained_agent.pt\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "save_path = os.path.join(path, filename)\n",
    "elite.saveCheckpoint(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
